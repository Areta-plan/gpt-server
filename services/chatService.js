// services/chatService.js
const fs = require('fs');
const path = require('path');
const { SYSTEM_PROMPT } = require('../prompts');
const { getTopKChunksByCategory, getTopKChunks } = require('../vectorStore');
const { webSearch } = require('./webSearch');
const { getOpenAIClient } = require('../lib/openaiClient');

// ìµœì‹  íŒŒì¸íŠœë‹ ëª¨ë¸ ID ì½ê¸° (ìë™ í•™ìŠµ í›„ latest_model.txtì— ì €ì¥)
function getLatestModel() {
  try {
    const content = fs.readFileSync(path.resolve(__dirname, '../latest_model.txt'), 'utf8').trim();
    
    // íŒŒì¼ ë‚´ìš©ì´ job ID í˜•íƒœë¼ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    if (!content || content.includes('Job ftjob-') || content.startsWith('#')) {
      console.log('ğŸ“‹ Using default model: gpt-4o-mini (no fine-tuned model available)');
      return 'gpt-4o-mini';
    }
    
    // ì‹¤ì œ ëª¨ë¸ IDì¸ ê²½ìš°
    console.log('ğŸ¤– Using fine-tuned model:', content);
    return content;
  } catch (error) {
    console.log('ğŸ“‹ Using default model: gpt-4o-mini (latest_model.txt not found)');
    return 'gpt-4o-mini';
  }
}

/**
 * ë‹¨ìˆœ ë©”ì‹œì§€ ì²˜ë¦¬: /ask ì—”ë“œí¬ì¸íŠ¸ìš©
 */
async function handleChatRequest(userMessage) {
  try {
    const openai = getOpenAIClient();
    const model = getLatestModel();
    
    // 1) ì„ë² ë”© ìƒì„±
    const { data } = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: userMessage
    });
    const embedding = data[0].embedding;

  // 2) RAG ì²­í¬ ì¶”ì¶œ
  const chunks = getTopKChunks(embedding, 3);
  console.log('---- RAG Debug (/ask) ----');
  console.log('User Message:', userMessage);
  console.log('Retrieved Chunks:', chunks);
  console.log('-------------------------');
  const knowledgeContext = chunks.join('\n\n');

  // 3) GPT í˜¸ì¶œ
  const messages = [
    { role: 'system', content: SYSTEM_PROMPT },
    { role: 'system', content: '[Knowledge]\n' + knowledgeContext },
    { role: 'user',   content: userMessage }
  ];
  console.log('Final Messages (/ask):', messages);
    const resp = await openai.chat.completions.create({
      model,
      messages,
      temperature: 0.3,
      top_p: 0.9,
      frequency_penalty: 0.7,
      max_tokens: 1800
    });
    return resp.choices[0].message.content;
  } catch (err) {
    console.error('[/ask] OpenAI call failed:', err.response?.data || err.message);
    
    if (err.code === 'insufficient_quota') {
      throw new Error('OpenAI API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê³„ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else if (err.code === 'rate_limit_exceeded') {
      throw new Error('API í˜¸ì¶œ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    }
    throw new Error('OpenAI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
  }
}

/**
 * ëŒ€í™” íˆìŠ¤í† ë¦¬ ì²˜ë¦¬: /chat ì—”ë“œí¬ì¸íŠ¸ìš©
 */
async function handleChatHistoryRequest(messages, references = []) {
  try {
    const openai = getOpenAIClient();
    const model = getLatestModel();
    const lastUser = [...messages].reverse().find(m => m.role === 'user');
    if (!lastUser) throw new Error('ì‚¬ìš©ì ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.');

    const { data } = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: lastUser.content
    });
    const embedding = data[0].embedding;

  const chunks = getTopKChunks(embedding, 3);
  const knowledgeContext = chunks.join('\n\n');

  const finalMessages = [
    { role: 'system', content: SYSTEM_PROMPT },
    { role: 'system', content: '[Knowledge]\n' + knowledgeContext },
    ...messages
  ];

  if (references.length > 0) {
    console.log('[chatService] uploaded references:', references.map(f => f.originalname));
  }

    console.log('Final Messages (/chat):', finalMessages);
    const resp = await openai.chat.completions.create({
      model,
      messages: finalMessages,
      temperature: 0.3,
      top_p: 0.9,
      frequency_penalty: 0.7,
      max_tokens: 1800
    });
    return resp.choices[0].message.content;
  } catch (err) {
    console.error('[/chat] OpenAI call failed:', err.response?.data || err.message);
    
    if (err.code === 'insufficient_quota') {
      throw new Error('OpenAI API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê³„ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else if (err.code === 'rate_limit_exceeded') {
      throw new Error('API í˜¸ì¶œ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    }
    throw new Error('ëŒ€í™” ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
  }
}

/**
 * ë¸”ë¡œê·¸ ì´ˆì•ˆ ìƒì„±: /blog ì—”ë“œí¬ì¸íŠ¸ìš©
 */
async function handleBlogRequest({ topic, mode, userParams }) {
  try {
    const openai = getOpenAIClient();
    const model = getLatestModel();

    // 1) ì›¹ ê²€ìƒ‰
    const results = await webSearch(`${topic} ìµœì‹  íŠ¸ë Œë“œ`, 3);
    const searchContext = results
      .map((r, i) => `${i+1}. ${r.title}\n${r.snippet}\n(${r.link})`)
      .join('\n\n');

    // 2) ì£¼ì œ ì„ë² ë”©
    const qEmb = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: topic
    });
    const emb = qEmb.data[0].embedding;

  // 3) ëª¨ë“ˆë³„ RAG ì»¨í…ìŠ¤íŠ¸
  const titlesCtx   = getTopKChunksByCategory(emb, 'titles', 3);
  const introCtx    = getTopKChunksByCategory(emb, 'intro', 3);
  const mainKey     = mode === 'story' ? 'mainStory' : 'mainKnowledge';
  const mainCtx     = getTopKChunksByCategory(emb, mainKey, 3);
  const strengthCtx = getTopKChunksByCategory(emb, 'strength', 3);
  const closingCtx  = getTopKChunksByCategory(emb, 'closing', 3);

  // 4) ì—…ë¡œë“œëœ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
  let uploadedContext = '';
  if (userParams.uploadedFiles && userParams.uploadedFiles.length > 0) {
    uploadedContext = userParams.uploadedFiles
      .map(file => `[${file.filename}]\n${file.content}`)
      .join('\n\n---\n\n');
  }

  console.log('---- RAG Debug (/blog) ----');
  console.log('Topic:', topic, 'Mode:', mode);
  console.log('User Params:', {
    target: userParams.target,
    tone: userParams.tone,
    brand: userParams.brand,
    style: userParams.style,
    uploadedFileCount: userParams.uploadedFiles?.length || 0
  });
  console.log('Search Context:', searchContext);
  console.log('Uploaded Files Context:', uploadedContext ? 'íŒŒì¼ ì—…ë¡œë“œë¨' : 'íŒŒì¼ ì—†ìŒ');
  console.log('Titles Context:', titlesCtx);
  console.log('Intro Context:', introCtx);
  console.log(`Main (${mode}) Context:`, mainCtx);
  console.log('Strength Context:', strengthCtx);
  console.log('Closing Context:', closingCtx);
  console.log('--------------------------');

  // 5) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
  let systemPrompt = `
${SYSTEM_PROMPT}

---
[ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­]
- ëŒ€ìƒ ë…ì: ${userParams.target}
- í†¤ì•¤ë§¤ë„ˆ: ${userParams.tone}
- ë¸Œëœë“œ: ${userParams.brand}
- ìŠ¤íƒ€ì¼: ${userParams.style}

[Web Search Results]
${searchContext}

1) [5 Compelling Titles ì°¸ê³  ìë£Œ]
${titlesCtx.join('\n\n')}

2) [First Paragraph ì°¸ê³  ìë£Œ]
${introCtx.join('\n\n')}

3) [Main Content (${mode}) ì°¸ê³  ìë£Œ]
${mainCtx.join('\n\n')}

4) [Brand Strength Highlight]
${strengthCtx.join('\n\n')}

5) [Emotional/Impactful Closing]
${closingCtx.join('\n\n')}`;

  // ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš© ì¶”ê°€
  if (uploadedContext) {
    systemPrompt += `\n\n[ì—…ë¡œë“œëœ ì°¸ê³  ìë£Œ]\n${uploadedContext}`;
  }

  systemPrompt += `\n\nìœ„ ìë£Œë“¤ì„ ì°¸ê³ í•´ì„œ, ì•„ë˜ êµ¬ì¡°ë¡œ ${userParams.target}ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ${userParams.tone} í†¤ì˜ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”:
1. 5 Compelling Titles
2. First Paragraph
3. Main Content (${mode})
4. Brand Strength Highlight (${userParams.brand})
5. Emotional/Impactful Closing`;

    // 6) GPT í˜¸ì¶œ
    const resp = await openai.chat.completions.create({
      model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user',   content: `ì£¼ì œ: ${topic}` }
      ],
      temperature: 0.25,
      top_p: 0.8,
      frequency_penalty: 0.7,
      max_tokens: 5000
    });
    return resp.choices[0].message.content;
  } catch (err) {
    console.error('[/blog] OpenAI call failed:', err.response?.data || err.message);
    
    if (err.code === 'insufficient_quota') {
      throw new Error('OpenAI API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê³„ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
    } else if (err.code === 'rate_limit_exceeded') {
      throw new Error('API í˜¸ì¶œ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
    }
    throw new Error('ë¸”ë¡œê·¸ ì´ˆì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
  }
}

module.exports = { handleChatRequest, handleChatHistoryRequest, handleBlogRequest };
